Data Analytics I
Create a Linear Regression Model using Python/R to predict home prices using Boston Housing
Dataset (https://www.kaggle.com/c/boston-housing). The Boston Housing dataset contains
information about various houses in Boston through different parameters. There are 506 samples
and 14 feature variables in this dataset.
The objective is to predict the value of prices of the house using the given features.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
* Linear Regression is a supervised machine learning algorithm used for predicting a continuous dependent variable based on one or more independent variables. It models the relationship between the variables by fitting a straight line (in the case of simple linear regression) to the observed data.
Y=Œ≤0 + Œ≤X + Œµ
Y: Dependent variable (target)
X: Independent variable (input)
ùõΩ0: Intercept of the line
ùõΩ1 : Slope of the line (regression coefficient)
Œµ: Error term (difference between actual and predicted value)

A. univarite - one independent variable to predict dependent variable
B. multivarite - multiple independent variables to predict dependent variable

* The Least Squares Method is a mathematical approach used in linear regression to find the best-fitting line by minimizing the sum of the squares of the residuals (errors).
‚àë(Yi ‚àí Y^i) ^ 2
Yi  = actual value
Y^i = predicted value from the line (y-cap)
(Yi ‚àí Y^i) =  residual (error)

* To predict how well our model predicts house prices , we use two key matrics 
i.  Mean Squared Error (MSE): It is a measure of the average squared difference between the actual and predicted values. A lower MSE indicates a better fit of the model.        It‚Äôs calculated as:
    MSE = 1/n ‚àë(Yi ‚àí Y^i) ^ 2
ii. R-squared Score (R¬≤): It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It is between 0 and 1, with 1 meaning perfect prediction
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
üîπ Part 1: Using Simple Dataset (Univariate Linear Regression)
Step 1: Importing Libraries
Import necessary Python libraries like:
NumPy for numerical calculations.
Pandas for handling data structures.
Matplotlib for visualizing the results.

Step 2: Creating the Dataset
Define input (independent) and output (dependent) variables.
Store the values in array format using NumPy.

Step 3: Building the Linear Regression Model
Use a method (like polyfit) to fit a line to the dataset.
The model generates coefficients (slope and intercept) representing the regression line(A regression line is a straight line that best fits the data in a scatter plot, showing the relationship between two variables).

Step 4: Observing Model Coefficients
Extract and display the slope and intercept of the regression line.

Step 5: Making Predictions
Use the regression model to predict output values for given inputs.

Evaluate the predicted output for a new input (e.g., predicting value when x = 65).

Step 6: Predicting for All Inputs
Generate predicted values for the entire dataset using the model.

This provides the model‚Äôs estimated outputs corresponding to each input.

Step 7: Evaluating Model Performance
Use performance metrics:

R¬≤ (R-squared) to determine how well the model explains the variance in the data.

A higher R¬≤ indicates a better-fitting model.

Step 8: Visualizing the Results
Plot the original data points.

Plot the regression line using the model.

Show predicted values as scatter points to compare against actual data.



üîπ Part 2: Using a Real Dataset (Boston Housing Dataset - Multivariate Regression)
Step 1: Importing Libraries
Import libraries for data handling (pandas), numeric processing (numpy), visualization (matplotlib), and machine learning (sklearn).

Step 2: Loading the Dataset
Load the Boston housing dataset from scikit-learn‚Äôs built-in datasets.

Step 3: Creating the Dataframe
Convert the dataset into a structured pandas DataFrame for easier analysis.

Step 4: Naming the Features
Assign feature names to the DataFrame columns to represent various housing attributes.

Step 5: Adding the Target Variable
Append the target variable (house prices) as a separate column in the DataFrame.

Step 6: Data Preprocessing
Check for missing or null values to ensure data quality before training.

Step 7: Defining Features and Target
Separate the input features (X) and the output target (y) from the dataset.

Step 8: Splitting Data
Divide the dataset into training and testing sets using a function like train_test_split.

Helps evaluate model performance on unseen data.

Step 9: Creating and Training the Model
Initialize and train the linear regression model using the training data.

The model learns the relationship between features and target.

Step 10: Making Predictions
Use the trained model to predict outputs for both training and testing data.

Step 11: Evaluating Model Performance
Compare actual and predicted outputs.

Create a data frame to display these comparisons for easier analysis.

Step 12: Calculating Performance Metrics
Calculate Mean Squared Error (MSE) and R-Squared (R¬≤) for both training and test sets to measure prediction accuracy.

Step 13: Visualizing Results
Plot true values vs predicted values for both training and test sets.

Use different colors and markers for clarity.

Visual comparison helps in understanding how well the model performs.
-------------------------------------------------------------------------------------------------------------------------------------------
IN CODE :-
1. y_pred = predict(x) -  This line assumes that predict() is a function you've defined or imported that takes input features x and returns predicted values y_pred.

2. from sklearn.metrics import r2_score
   r2_score(y, y_pred)
y is the array of true values (actual target values).
y_pred is the array of predicted values from your model.
This function returns a value between 0 and 1
1.0 = perfect prediction,
0.0 = model does no better than the mean of the target,
< 0.0 = model performs worse than the mean.

3. y_line = model[1] + model[0] * x
This creates the regression line 
ùë¶=ùëöùë•+ùëê
y=mx+c, assuming:
model[0] is the slope (m),
model[1] is the intercept (c).
4. plt.plot(x, y_line, c='r')
This plots the red regression line.
5.plt.scatter(x, y_pred)
This plots the actual data points in red.

6. data.isnull().sum()
data.isnull() returns a DataFrame of the same shape with True for missing values.
.sum() adds up the True values column-wise, showing the number of missing values per column

7. x = data.drop(['PRICE'], axis=1)
y = data['PRICE']
is splitting the dataset into:
x: all features (independent variables) except the 'PRICE' column.
y: the target variable, which is 'PRICE'.
axis=1 ‚Üí perform the operation column-wise

8. ytrain_pred = lm.predict(xtrain)
ytest_pred = lm.predict(xtest)
are using the trained model lm (likely a LinearRegression or similar model from scikit-learn) to generate predictions on:
xtrain: training features ‚Üí produces ytrain_pred
xtest: testing features ‚Üí produces ytest_pred

9. from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=0)

i. test_size=0.2: 20% of the data will be used as the test set, and 80% as     the training set.

ii. random_state=0: Ensures that the split is reproducible every time you        run the code.

This prepares your data for training and evaluation.
Now you're all set to:

1. Fit your model on xtrain, ytrain.
2. Predict on xtest.
3. Evaluate performance using r2_score, mean_squared_error, etc.
